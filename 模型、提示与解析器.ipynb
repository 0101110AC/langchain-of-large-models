{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地.env文件\n",
    "\n",
    "# 初始化OLLAMA模型，指定模型名称和本地服务地址\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "The sum of 1 and 1 is **2**.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    # 创建一个提示模板\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"input_text\"],\n",
    "        template=\"{input_text}\"\n",
    "    )\n",
    "    # 创建一个LLMChain，将模型和提示模板关联起来\n",
    "    chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "    # 运行LLMChain并返回结果\n",
    "    return chain.run({\"input_text\": prompt})\n",
    "\n",
    "# 使用OLLAMA模型生成文本\n",
    "response = get_completion(\"what is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to translate the given text into American English style with a calm and respectful tone. The user provided an example where they translated \"Arrr,I be fuming that me blender lid flew off...\" into a more conversational tone.\n",
      "\n",
      "First, I'll read through the original text carefully:\n",
      "\n",
      "\"Arrr,I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen walls. I need yer help matey!\"\n",
      "\n",
      "I notice a few things: the opening is in an unusual casing (three backticks), which might be specific to the platform. The sentences are a bit repetitive and have some casual phrases like \"I need yer help matey\" which sounds more friendly than formal.\n",
      "\n",
      "In American English, it's important to keep the tone respectful and polite, not too informal. I should avoid contractions that make the sentences feel forced. Also, keeping the language flowing smoothly without abrupt changes in structure is key.\n",
      "\n",
      "Looking at each line:\n",
      "\n",
      "1. \"Arrr,I be fuming that me blender lid flew off...\" – The opening sounds a bit odd with the casing and some repetitive phrases. Translating it to a more conversational tone would make it feel more natural.\n",
      "\n",
      "2. \"And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen walls.\" – This can be rephrased for clarity. Instead of saying \"I need yer help matey,\" perhaps something like \"Please let me know\" or \"You're welcome.\"\n",
      "\n",
      "3. \"I need yer help matey!\" – The exclamation at the end might seem too strong, so I should soften it to a more polite conclusion.\n",
      "\n",
      "Putting it all together, I want each line to flow naturally with American English phrases and tones. I'll also ensure consistency in sentence structure for smooth reading.\n",
      "</think>\n",
      "\n",
      "Here’s the translated text in American English style, maintaining a calm and respectful tone:\n",
      "\n",
      "\"Arrr, I’m be fuming that my blender lid went off and splattered me kitchen walls with a smoothie! And to make matters worse, the warranty won’t cover the cost of cleaning up those kitchen walls. Please let me know if this makes you any trouble.\"\n",
      "\n",
      "This translation keeps the original message clear while adopting a more natural, polite, and conversational tone suitable for American English communication.\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"Arrr,I be fuming that me blender lid\\\n",
    "    flew off and splattered me kitchen walls\\\n",
    "    with smoothie! And to make matters worse,\\\n",
    "    the warranty don't cover the cost of\\\n",
    "    cleaning up me kitchen walls. I need yer help\\\n",
    "    right now, matey!\n",
    "    \"\"\"\n",
    "\n",
    "style = \"\"\"American English\\\n",
    "    in a calm and respectful tone\\\n",
    "    \"\"\"\n",
    "\n",
    "# 创建一个提示模板，将客户邮件和风格作为输入变量，并指定输出变量为翻译后的文本。\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\\\n",
    "into a style that is {style}.\\\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "# 使用OLLAMA模型生成翻译后的文本\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=' '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "# 使用Ollama本地模型时，如果需要调节模型温度，需要在创建时就进行设置\n",
    "# 如果要在运行中修改模型温度，则需要重新创建OllamaLLM实例\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\",base_url=\"http://localhost:11434\",temperature=0.1)\n",
    "\n",
    "# 使用消息模板列表初始化ChatPromptTemplate\n",
    "# 因为ChatPromptTemplate必须要一个消息模板列表作为参数，\n",
    "# 无法打印空的ChatPromptTemplate实例，所以这里打印一个空格字符串\n",
    "\n",
    "chat = ChatPromptTemplate.from_messages([\" \"])\n",
    "\n",
    "# 直接打印chat实例\n",
    "print(chat) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input_text', 'style'], input_types={}, partial_variables={}, template='Translate the text     that is delimited by triple backticks     into a style that is {style}.    text: ```{input_text}```')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "    that is delimited by triple backticks \\\n",
    "    into a style that is {style}.\\\n",
    "    text: ```{input_text}```\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_text', 'style']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
