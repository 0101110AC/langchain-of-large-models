{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())  # 读取本地.env文件\n",
    "\n",
    "# 初始化OLLAMA模型，指定模型名称和本地服务地址\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33249\\AppData\\Local\\Temp\\ipykernel_38228\\3423226809.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=model, prompt=prompt_template)\n",
      "C:\\Users\\33249\\AppData\\Local\\Temp\\ipykernel_38228\\3423226809.py:10: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return chain.run({\"input_text\": prompt})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "1 + 1 equals **2**.\n"
     ]
    }
   ],
   "source": [
    "def get_completion(prompt, model=model):\n",
    "    # 创建一个提示模板\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"input_text\"],\n",
    "        template=\"{input_text}\"\n",
    "    )\n",
    "    # 创建一个LLMChain，将模型和提示模板关联起来\n",
    "    chain = LLMChain(llm=model, prompt=prompt_template)\n",
    "    # 运行LLMChain并返回结果\n",
    "    return chain.run({\"input_text\": prompt})\n",
    "\n",
    "# 使用OLLAMA模型生成文本\n",
    "response = get_completion(\"what is 1+1?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user wants me to translate a specific text into American English style. The text is enclosed in triple backticks and has some pretty strong words. Let's see... The original text is about someone blowing off their blender lid, splattering walls with smoothies, and complaining that the warranty doesn't cover cleanup costs.\n",
      "\n",
      "First, I need to identify the key phrases: \"blender lid,\" \"splattered me kitchen walls,\" \"warranty don't cover.\" These are critical because they convey frustration. The user wants this translated into a calm and respectful tone, so I'll need to soften the language while keeping the emotions intact.\n",
      "\n",
      "\"Blender lid\" might be a bit harsh. Maybe something like \"blender handle\" or \"blender cap\" could work, but in an American context, using \"blender handle\" feels more natural. It's also less severe and maintains the essence of the original frustration without being too intense.\n",
      "\n",
      "\"splattered me kitchen walls\" is pretty dark. To make it softer, I can say something like \"wiped my kitchen counter.\" That makes it more palatable and reduces the impact on the person reading.\n",
      "\n",
      "\"The warranty don't cover...\" This part is strong because it expresses a problem with the product. In an American tone, it's better to acknowledge that it's frustrating but not impossible. So instead of saying \"doesn't cover,\" I can use \"won't cover\" which sounds more respectful and acknowledges the issue without sounding confrontational.\n",
      "\n",
      "I should also make sure the overall sentence flows naturally. The user is clearly in a situation where they're dealing with a problem, so keeping it open-ended but empathetic will be key. Maybe ending with something like \"Don’t worry,\" to show that while it's frustrating, it’s not impossible. This makes the response empathetic and acknowledges their frustration.\n",
      "\n",
      "Putting it all together: \"Blender handle, I be fuming! Wiped my kitchen counter with my favorite smoothie mix. And no, won't cover the cost of cleaning up now.\" It maintains the original tone but softens each word, making it more approachable and less intimidating.\n",
      "</think>\n",
      "\n",
      "Arrr,I be fuming that me blender handle flew off and splattered me kitchen counter with my favorite smoothie mix! And to make matters worse, no, won't cover the cost of cleaning up now. I need yer help matey!\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"Arrr,I be fuming that me blender lid\\\n",
    "    flew off and splattered me kitchen walls\\\n",
    "    with smoothie! And to make matters worse,\\\n",
    "    the warranty don't cover the cost of\\\n",
    "    cleaning up me kitchen walls. I need yer help\\\n",
    "    right now, matey!\n",
    "    \"\"\"\n",
    "\n",
    "style = \"\"\"American English\\\n",
    "    in a calm and respectful tone\\\n",
    "    \"\"\"\n",
    "\n",
    "# 创建一个提示模板，将客户邮件和风格作为输入变量，并指定输出变量为翻译后的文本。\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\\\n",
    "into a style that is {style}.\\\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "# 使用OLLAMA模型生成翻译后的文本\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_text'], input_types={}, partial_variables={}, template='{input_text}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "# 使用Ollama本地模型时，如果需要调节模型温度，需要在创建时就进行设置\n",
    "# 如果要在运行中修改模型温度，则需要重新创建OllamaLLM实例\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\",base_url=\"http://localhost:11434\",temperature=0.1)\n",
    "\n",
    "# 使用消息模板列表初始化ChatPromptTemplate\n",
    "# 因为ChatPromptTemplate必须要一个消息模板列表作为参数，\n",
    "# 无法打印空的ChatPromptTemplate实例，所以这里打印一个空格字符串\n",
    "\n",
    "# 定义一个 HumanMessagePromptTemplate\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(\"{input_text}\")\n",
    "\n",
    "# 使用消息模板列表初始化 ChatPromptTemplate\n",
    "chat = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "\n",
    "# 直接打印chat实例\n",
    "print(chat) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input_text', 'style'], input_types={}, partial_variables={}, template='Translate the text     that is delimited by triple backticks     into a style that is {style}.    text: ```{input_text}```')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "    that is delimited by triple backticks \\\n",
    "    into a style that is {style}.\\\n",
    "    text: ```{input_text}```\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_text', 'style']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "content=\"Translate the text     that is delimited by triple backticks     into a style that is  American English    in a calm and respectful tone    .    text: ```Arrr,I be fuming that me blender lid    flew off and splattered me kitchen walls    with smoothie! And to make matters worse,    the warranty don't cover the cost of    cleaning up me kitchen walls. I need yer help    right now, matey!\\n    ```\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\" American English\\\n",
    "    in a calm and respectful tone\\\n",
    "    \"\"\"\n",
    "\n",
    "customer_email = \"\"\"Arrr,I be fuming that me blender lid\\\n",
    "    flew off and splattered me kitchen walls\\\n",
    "    with smoothie! And to make matters worse,\\\n",
    "    the warranty don't cover the cost of\\\n",
    "    cleaning up me kitchen walls. I need yer help\\\n",
    "    right now, matey!\n",
    "    \"\"\"\n",
    "    \n",
    "customer_messages = prompt_template.format_messages(\n",
    "    style = customer_style,\n",
    "    input_text = customer_email\n",
    ")\n",
    "\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I have this query where someone wants me to translate a text delimited by triple backticks into American English with a calm and respectful tone. The text is pretty short and has some casual phrases.\n",
      "\n",
      "First, I need to understand the context. The user provided an example response that translated the text into a more formal and polite manner. They used words like \"arrr\" which is slang for \"arrrr,\" so maybe they meant to use that but it's not standard. Then, they changed \"fuming\" to \"fogging\" which seems like a typo or intentional change in tone.\n",
      "\n",
      "I should consider the user's intent. They probably want the message to sound more professional and respectful, especially since it's addressed to someone else. The original text is a bit informal with some slang and casual language, so the translation needs to make that sound more polished.\n",
      "\n",
      "Looking at the example response, they replaced \"fuming\" with \"fogging,\" which makes sense as a typo correction but also changes the tone from a bit harsher to more gentle. So, I should follow that approach: correct any typos and adjust the language to be more formal yet still respectful.\n",
      "\n",
      "I'll start by identifying each line. The first line is \"Arrr,I be fuming that me blender lid flew off...\" I can change \"fuming\" to \"fogging.\" Then, \"splattered me kitchen walls with smoothie!\" could become \"spilled my smoothie on the kitchen walls,\" which sounds more formal.\n",
      "\n",
      "The second part says, \"And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen walls.\" I'll rephrase that as \"And it's not covered by the warranty for cleaning up my kitchen walls.\" That keeps the tone consistent and polite.\n",
      "\n",
      "Finally, the closing line is \"I need yer help right now, matey!\" which should be changed to \"I need your assistance immediately,\" to sound more urgent but still respectful. Using \"your\" instead of \"you\" makes it a bit more formal in addressing someone else's request.\n",
      "\n",
      "Putting it all together, I'll make sure each line flows naturally and maintains the calm and respectful tone while correcting any grammatical issues or typos. This should meet the user's requirements effectively.\n",
      "</think>\n",
      "\n",
      "The text delimited by triple backticks has been translated into a style that is American English with a calm and respectful tone. Here's how it was rephrased:\n",
      "\n",
      "```Arrr,I fogging that me blender lid spilled my smoothie on the kitchen walls And it's not covered by the warranty for cleaning up my kitchen walls. I need your assistance immediately, matey!\n",
      "```\n",
      "\n",
      "The translation maintains a formal yet polite tone while correcting any grammatical errors or typos in the original text.\n"
     ]
    }
   ],
   "source": [
    "# ChatOpenAI 实现了 __call__ 方法，所以可以像函数一样被调用，用于和 OpenAI 的聊天模型交互。\n",
    "\n",
    "# ChatPromptTemplate 主要用于构建和格式化提示消息，没有实现 __call__ 方法，不能直接调用，\n",
    "# 需要先使用 format_messages 方法生成消息列表，再把消息列表传递给可调用的模型对象（如 ChatOpenAI 或 Ollama）来获取响应。\n",
    "\n",
    "# invoke 方法是 langchain 里模型类提供的一个用于触发模型推理的方法。\n",
    "# 当你调用 model.invoke(customer_messages) 时，实际上是在执行以下操作：\n",
    "# 封装请求、发送请求、处理响应、返回结果。\n",
    "\n",
    "#使用 model 与模型交互\n",
    "customer_response = model.invoke(customer_messages)\n",
    "print(customer_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text     that is delimited by triple backticks     into a style that is     a polite tone     that speaks in English Pirte     .    text: ```Hey there customer,    the warranty does not cover     cleaning expenses for your kitchen     because it's your fault that     you misused your blender     by forgetting to put the lid on before     starting the blender.     Tough luck ! See ya !    \n",
      "    ```\n",
      "<think>\n",
      "Okay, so I need to translate this text into a polite English style. Let me read through it again.\n",
      "\n",
      "The user wrote: \"Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck ! See ya !\" \n",
      "\n",
      "Hmm, I notice some punctuation issues here. The closing quote is missing a period and a space after \"See ya!\" Maybe that should be \"See ya\" instead? Also, the word \"tough luck\" seems a bit informal. In polite English, it's better to use \"good luck\" or \"thank you.\" \n",
      "\n",
      "I also see that the user used \"by forgetting,\" which is correct but could be phrased more politely as \"without putting the lid on before starting the blender.\" That sounds smoother and less harsh.\n",
      "\n",
      "So, I'll rephrase each part. Replace the closing quote with a period and space, change \"tough luck\" to \"good luck,\" and make the sentence flow better. Let me put it all together now.\n",
      "</think>\n",
      "\n",
      "Certainly! Here's the translated text in a polite English style:\n",
      "\n",
      "\"Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Good luck! See ya!\"\n"
     ]
    }
   ],
   "source": [
    "service_reply = \"\"\"Hey there customer,\\\n",
    "    the warranty does not cover \\\n",
    "    cleaning expenses for your kitchen \\\n",
    "    because it's your fault that \\\n",
    "    you misused your blender \\\n",
    "    by forgetting to put the lid on before \\\n",
    "    starting the blender. \\\n",
    "    Tough luck ! See ya !    \n",
    "    \"\"\"\n",
    "    \n",
    "service_style_pirate = \"\"\"\\\n",
    "    a polite tone \\\n",
    "    that speaks in English Pirte \\\n",
    "    \"\"\"\n",
    "    \n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    input_text=service_reply\n",
    ")\n",
    "\n",
    "print(service_messages[0].content)\n",
    "\n",
    "service_response = model.invoke(service_messages)\n",
    "print(service_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable !'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"gift\":False,\n",
    "    \"delivery_days\":5,\n",
    "    \"price_value\":\"pretty affordable !\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "    This leaf blower is pretty affordable \\\n",
    "    and does the job pretty well. \\\n",
    "    I would buy anthor one  for my friends Tom . \\\n",
    "    who needs a leaf blower. \\\n",
    "    It will take 2 days to arrive. \\\n",
    "    \"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "    For the following text , extract the following information:\n",
    "    things: what is the product?\n",
    "    gift: is it a gift?\n",
    "    delivery_days: how many days does it take to deliver?\n",
    "    \n",
    "    Format the output as JSON with the following keys:\n",
    "    things\n",
    "    gift\n",
    "    delivery_days\n",
    "    \n",
    "    text: {text}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    For the following text , extract the following information:\n",
      "    things: what is the product?\n",
      "    gift: is it a gift?\n",
      "    delivery_days: how many days does it take to deliver?\n",
      "    \n",
      "    Format the output as JSON with the following keys:\n",
      "    things\n",
      "    gift\n",
      "    delivery_days\n",
      "    \n",
      "    text:     This leaf blower is pretty affordable     and does the job pretty well.     I would buy anthor one  for my friends Tom .     who needs a leaf blower.     It will take 2 days to arrive.     \n",
      "    \n",
      "<think>\n",
      "Okay, so I need to extract information from this text about the product, whether it's a gift, and how many delivery days there are. Let me read through the text carefully.\n",
      "\n",
      "The text starts with \"This leaf blower is pretty affordable\" – that seems like the price aspect. Then it says \"and does the job pretty well.\" That probably means it's good for its price. Next, it mentions \"I would buy anthor one  for my friends Tom . who needs a leaf blower.\" So, I think \"anthor\" is the brand name here.\n",
      "\n",
      "The text continues by saying \"It will take 2 days to arrive.\" That gives me the delivery time. \n",
      "\n",
      "So putting that together: things include price and performance. It's not a gift because it's being sold as a product for personal use. The delivery_days are two.\n",
      "</think>\n",
      "\n",
      "things: pretty affordable, does the job pretty well  \n",
      "gift: no  \n",
      "delivery_days: 2\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate,HumanMessagePromptTemplate\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\",base_url=\"http://localhost:11434\",temperature=0.1)\n",
    "\n",
    "# 调用format_messages时要为模板中的所有占位符（即{text},{extra_info}）传入对应的值，\n",
    "# 这样才能保证messages具有完整的内容\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "print(messages[0].content)\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseSchema(name='things', description='what is the product?', type='string'), ResponseSchema(name='gift', description='is it a gift?', type='string'), ResponseSchema(name='delivery_days', description='how many days does it take to deliver?', type='string')]\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "things_schema = ResponseSchema(name=\"things\",description=\"what is the product?\")\n",
    "gift_schema = ResponseSchema(name=\"gift\",description=\"is it a gift?\") #  定义一个名为 gift_schema 的 ResponseSchema 对象，用于描述是否为礼物\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",description=\"how many days does it take to deliver?\") #  定义一个名为 delivery_days_schema 的 ResponseSchema 对象，用于描述配送所需天数\n",
    "\n",
    "response_schemas = [things_schema,gift_schema,delivery_days_schema] #  创建一个列表 response_schemas，包含之前定义的 things_schema、gift_schema 和 delivery_days_schema\n",
    "print(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"things\": string  // what is the product?\n",
      "\t\"gift\": string  // is it a gift?\n",
      "\t\"delivery_days\": string  // how many days does it take to deliver?\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# 实例化 StructuredOutputParser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "    For the following text , extract the following information:\n",
    "    \n",
    "    things: what is the product?\n",
    "    gift: is it a gift?\n",
    "    delivery_days: how many days does it take to deliver?\n",
    "\n",
    "    Format the output as JSON with the following keys:\n",
    "    things\n",
    "    gift\n",
    "    delivery_days\n",
    "\n",
    "    text: {text}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "prompt_template_2 = ChatPromptTemplate.from_template(template=review_template_2) #  使用ChatPromptTemplate类从上述模板创建一个聊天提示模板对象\n",
    "messages = prompt_template_2.format_messages( #  使用format_messages方法格式化消息，传入客户评论和格式指令 customer_review是客户提供的评论文本 format_instructions是格式化输出的指令\n",
    "            text=customer_review,format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    For the following text , extract the following information:\n",
      "    \n",
      "    things: what is the product?\n",
      "    gift: is it a gift?\n",
      "    delivery_days: how many days does it take to deliver?\n",
      "\n",
      "    Format the output as JSON with the following keys:\n",
      "    things\n",
      "    gift\n",
      "    delivery_days\n",
      "\n",
      "    text:     This leaf blower is pretty affordable     and does the job pretty well.     I would buy anthor one  for my friends Tom .     who needs a leaf blower.     It will take 2 days to arrive.     \n",
      "    The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"things\": string  // what is the product?\n",
      "\t\"gift\": string  // is it a gift?\n",
      "\t\"delivery_days\": string  // how many days does it take to deliver?\n",
      "}\n",
      "```\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'things': 'leaf blower', 'gift': False, 'delivery_days': 2}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    # 调用模型生成响应\n",
    "    response_text = model.predict(messages[0].content) #  使用模型的predict方法生成响应文本\n",
    "\n",
    "    # 提取 JSON 部分\n",
    "    json_pattern = r'```json(.*?)```' #  定义正则表达式模式，用于匹配以```json开头和结尾的部分\n",
    "    match = re.search(json_pattern, response_text, re.DOTALL) #  在响应文本中搜索匹配的部分，re.DOTALL使.匹配包括换行符在内的所有字符\n",
    "\n",
    "    if match:\n",
    "        json_text = match.group(1).strip() #  提取匹配到的JSON文本，并去除首尾的空白字符\n",
    "        output_dict = output_parser.parse(f\"```json\\n{json_text}\\n```\") #  使用output_parser解析JSON文本\n",
    "        print(output_dict) #  打印解析后的字典\n",
    "    else:\n",
    "        print(\"未找到有效的 JSON 部分。\") #  如果未找到匹配的JSON部分，打印提示信息\n",
    "except Exception as e:\n",
    "    print(f\"发生错误: {e}\") #  捕获并打印任何异常信息\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(type(output_dict))\n",
    "print(output_dict.get(\"gift\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
