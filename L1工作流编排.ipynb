{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runable 协议"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>|\n",
      "|嗯|，|用户|问|“|天空|是什么|颜色|”，|我|得|先|想想|这个问题|。|首先|，|天空|本身|是不是|颜色|呢|？|我记得|有时候|天空|看起来|像|淡|蓝色|，|但|其实|它|不是|直接|的颜色|。|可能|是因为|光|的|折射|和|散|射|，|使得|光线|显得|比较|柔和|，|而不是|纯粹|的颜色|。\n",
      "\n",
      "|接下来|，|颜色|这个词|本身|指的是|物体|对|可见|光|的能量|有|吸收|或|反射|的能力|。|而|天空|是否|真的|“|颜色|”|还要|看|具体情况|。|比如|，|如果|我在|下雨|天|，|雨|点|会|发出|不同|颜色|的|光|，|那|天空|的颜色|可能|与|雨|点|的颜色|有关|，|或者|我|看到|的颜色|更多|是|周围的|空气|里的|光线|。\n",
      "\n",
      "|还有|，|有时候|天空|看起来|很|蓝|，|这是因为|地球|上的|大气|层|吸收|了|蓝|光|，|让|光线|更|柔和|地|反射|回去|，|形成|一片|蓝| sky|。|这种|现象|叫做|天|蓝色|，|而不是|颜色|本身|。\n",
      "\n",
      "|另外|，|如果|在|月|夜|的时候|，|天空|可能|看起来|像|黑色|的|，|因为|没有|空气中|气体|的|散|射|和|折射|，|所以|更多的|光线|被|吸收|，|导致|整个|天空|比较|暗|淡|。|但这|只是|我的|个人|感受|，并|不是|科学|上的|结论|。\n",
      "\n",
      "|我还|得|考虑|一下|，|人类|如何|感知|颜色|。|我们的|视觉|系统|无法|直接|感知|“|颜色|”|本身|，|而是|通过|反射|、|折射|和|色|散|来|呈现|物体|的颜色|。|因此|，“|颜色|”|并不是|一种|独立|的存在|，|而|是一种|通过|光线|反射|的现象|。\n",
      "\n",
      "|另外|，|天空|的颜色|可能会|随|时间|变化|，|比如|春天|天|更|蓝|，|夏天|天|更|绿|，|秋天|天|更|黄|，|冬天|天|更|白|。|这些|现象|也|与|季节|的变化|有关|，|而不是|“|颜色|”的|本身|。\n",
      "\n",
      "|还有|，|天空|的颜色|可能|受到|天气|条件|的影响|，|比如|下雨|天|、|阴|天|、|晴|天|等|，|都会|影响|天空|的颜色|看起来|如何|。\n",
      "\n",
      "|所以|，|回到|用户|的问题|，“|天空|是什么|颜色|”，|我|需要|解释|一下|，|因为|天空|不是一个|直接|的颜色|，|而|是一种|光线|通过|大气|层|折射|和|散|射|后的|结果|。|颜色|是|物体|对|可见|光|的能量|有|吸收|或|反射|的能力|，|而|天空|的颜色|则是|由于|地球|环境|和|光线|的|特殊|作用|所|呈现|的现象|。\n",
      "|</think>|\n",
      "\n",
      "|天空|并不是|一种|真正|存在的|“|颜色|”。|颜色|是|物体|对|可见|光|能量|的|吸收|或|反射|能力|，|而非|直接|显示|在|天空|中的|现象|。|天空|看起来|“|颜色|”|主要是|由|地球|大气|层|的作用|引起|。|比如|，|下雨|天|的|天空|因为|雨|滴|释放|的颜色|，|但|整体|看|来|更多|是|光线|被|折射|和|散|射|后|形成|的一种|柔和|效果|。|在|月|夜|时|，|天空|几乎|透明|，|显得|暗|淡|，|这是|由于|大气|层|吸收|光|能|所致|。|此外|，|天空|的颜色|还会|随|时间和|季节|变化|而|改变|。|因此|，“|颜色|”|这一|概念|不是|天空|本身|所|具备|的|性质|，|而是|通过|光线|折射|、|反射|和|散|射|后|形成的|视觉|效果|。||\n",
      " ['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center', 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format', 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal', 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'maketrans', 'partition', 'removeprefix', 'removesuffix', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for chunk in model.stream(\"天空是什么颜色\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk, end=\"|\", flush=True)\n",
    "    # print(chunk.content, end=\"|\", flush=True)\n",
    "    \n",
    "print(\"\\n\",dir(chunk)) # chunk 是一个字符串类型，并不像OpenAI的ChatOpenAI那样有content属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('问', '”，', '这个问题')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[5],chunks[10],chunks[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 异步调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "import asyncio\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# ChatPromptTemplate.from_messages()期望传入的格式极为严格，\n",
    "# 每个message必须包含role和content属性，并且content属性必须是一个字符串。\n",
    "# 如果传入的message不满足这些要求，ChatPromptTemplate.from_messages()会抛出一个ValueError异常。\n",
    "\n",
    "\n",
    "# 修改这部分，使用HumanMessagePromptTemplate创建符合格式的消息\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"给我讲一个关于{topic}的故事\")\n",
    "])\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "# parser = StrOutputParser()\n",
    "# chain = prompt | model | parser\n",
    "chain = prompt | model\n",
    "\n",
    "async def async_stream():\n",
    "    async for chunk in chain.stream({\"topic\":\"天空\"}):\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "\n",
    "# asyncio.run(async_stream()) # chain.stream({\"topic\":\"天空\"}) 返回的是一个普通的生成器对象，\n",
    "# 而不是一个实现了异步迭代协议（即具有 __aiter__ 和 __anext__ 方法）的异步可迭代对象\n",
    "\n",
    "# 因此，在调用 asyncio.run() 时，Python 会尝试将生成器对象转换为异步可迭代对象，\n",
    "# 但由于生成器对象本身并不实现异步迭代协议，因此会引发 TypeError 异常。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>|\n",
      "|嗯|，|用户|让我|给|一个|关于|天空|的故事|。|这|可能|是一个|挺|有趣的|请求|，|因为|天空|通常|是我们|比较|熟悉|的概念|，|但|有时候|我们可以|忽略|它的|伟大|和|独特|之处|。\n",
      "\n",
      "|首先|，|我|需要|确定|故事|的主题|是什么|。|可能是|从|自然|元素|开始|，|比如|星星|、|阳光|或者|云|朵|，|这样|更容易|引起|读者|的兴趣|。|另外|，|主角|的|设定|也很|重要|，|比如|他|来自|哪里|，|遇到|什么|特别|的事情|或者|事件|，|这样|故事|会|更|生动|有趣|。\n",
      "\n",
      "|考虑到|用户|可能|希望|故事|有|深度|，|我可以|加入|一些|象征|意义|或|情感|元素|。|例如|，|天空|中的|某个|地方|可能|代表|一个|特定|的人|、|文化|或|历史|背景|，|赋予|了|它|独特的| meaning|。\n",
      "\n",
      "|接下来|，|我会|构思|一个|简单|但|富有|画面|感|的故事|。|比如|，|主角|来自|一个小| town|，|那里|有一|片|荒|地|，|后来|变成了|一个|重要的|自然|保护区|。|这样|，|故事|不仅|讲述了|天空|的主题|，|还|融入|了|个人|的经历|和|情感|，|让|读者|更容易|产生|共鸣|。\n",
      "\n",
      "|在|语言|风格|上|，|要|保持|口语|化|，|避免|过于|正式|或|学术|化的|表达|，|让|故事|更|易于|理解和|接受|。|同时|，|通过|细腻|的|描写|，|如|天气|的变化|、|动物|的行为|，|来|生动|描绘|天空|的存在|和|影响|。\n",
      "\n",
      "|最后|，|确保|故事|有一个|引|人|入|胜|的|开头|，|点|明|主题|，并|逐渐|展开|情节|，|引导|读者|跟随|主角|的情感|和|经历|。|这样|，|一个|关于|天空|的故事|就能|既|符合|用户的|请求|，|又能|引发|读者|的兴趣|。\n",
      "|</think>|\n",
      "\n",
      "|好的|！|让我们|讲|一个|关于|天空|的故事|——|关于|一个|特别|的地方|和|它的|伟大|之处|。\n",
      "\n",
      "|---\n",
      "\n",
      "|在一个|小| towns|，|有一|片|荒|凉|的|的土地|。|那里|有一|块|巨大的|岩石|，在|阳光|下|闪闪|发|亮|。|这块|岩石|就是|我们|今天|要|讲|的那个|地方|。\n",
      "\n",
      "|那|年的|夏天|，|天空|格外|蓝|。|它|像|是一位|神秘|的|守护|神|，|为|这个|美丽的|大陆|增添|了一|丝|宁静|。|每个月|都有|星星|闪烁|，|就像|无数|个|眼睛|在|注视|着|大地|。|远处|的|山|峦|被|月|光|染|成|银|白|，在|夜晚|显得|更加|迷|离|。|风|轻轻|拂|过|脸颊|，|带着|露|珠|的|清香|。\n",
      "\n",
      "|每年|夏|至|，|这片|岩石|都会|举办|一场|盛|大的|仪式|。|人们|在这里|种|下了|各种|野|花|，|种|下了|自己的|希望|。|传说|中|，|这|颗|巨|石|是|祖|父母|留给|他们的|礼物|，|让他们|可以|远离|家乡|，|享受|自然|的|馈|赠|。\n",
      "\n",
      "|有一天|，|一位|老|者|来到|这里|。|他的|妻子|去世|了|，|他|一直在|寻找|一个|家|的感觉|。|他|走|来|时|，|只|看见|天空|湛|蓝|如|洗|，|阳光|温柔|地|洒|在|岩石|上|。|他|轻|声|说|：“|天|上有|你|，|那里|就有|你的|安宁|。”\n",
      "\n",
      "|从|那天|起|，|这片|岩石|开始|变得|神秘|起来|。|每年|夏|至|，|人们|都会|在那里|种|花|庆祝|。|不过|，|后来|发生|了一|件事|，|这|颗|巨|石|被|破坏|了|。\n",
      "\n",
      "|那|年|冬天|，|风|起了|。|岩石|的|裂缝|慢慢|打开|，|钻|出了|一个|巨大的|山|洞|。|洞|口|处|有一个|巨大的|金属|球|滚|了出来|。|它|散发着|刺|目的|白|光|，|就像|一群|小|兔子|在|跳|着|舞|步|。\n",
      "\n",
      "|老|者|站在|洞|口|，|看着|那|发|亮|的|金属|球|。|他|颤抖|着|说|：“|我|看到|什么东西|了|。”|但|等|他|把|目光|投|向|天空|时|，|却|看到|一个|美丽的|月亮|正在|升起|。\n",
      "\n",
      "|那|个月|亮|像|是一|颗|闪烁|的眼睛|，|照亮|了他的|世界|。|他知道|，|那|里的|天空|不再|只是|阳光|下的|星空|，|而是|盛|满了|希望|和|爱|。\n",
      "\n",
      "|---\n",
      "\n",
      "|这个|故事|中|，|天空|不仅是|天|上的|风景|，|更是|连接|着|人们|的心|灵|的|桥梁|。|它|让我们|感受到|生命的|伟大|和|永恒|的力量|。|也许|这就是| why| 我|们|常|以|天空|为|象征|，|不仅仅|是为了|欣赏|它|本身|，|更|为了|见证|它|赋予|我们|的一切|意义|。||"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "# 包装生成器为异步可迭代对象的类\n",
    "class AsyncGeneratorWrapper:\n",
    "    def __init__(self, gen):\n",
    "        # 初始化时传入一个生成器对象\n",
    "        self.gen = gen\n",
    "\n",
    "    def __aiter__(self):\n",
    "        # 使该类的实例成为异步可迭代对象\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        try:\n",
    "            # 使用 asyncio.to_thread 避免阻塞事件循环，将同步的 next 调用放到单独线程执行\n",
    "            return await asyncio.to_thread(next, self.gen)\n",
    "        except StopIteration:\n",
    "            # 当生成器耗尽时，抛出异步迭代结束的异常\n",
    "            raise StopAsyncIteration\n",
    "        except Exception as e:\n",
    "            # 捕获并打印其他可能出现的异常\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            # 出现异常时也结束异步迭代\n",
    "            raise StopAsyncIteration\n",
    "\n",
    "\n",
    "# 创建一个聊天提示模板，接收一个 topic 参数，用于生成用户的提问消息\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    HumanMessagePromptTemplate.from_template(\"给我讲一个关于{topic}的故事\")\n",
    "])\n",
    "\n",
    "# 初始化 Ollama 语言模型，指定模型名称和服务地址\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "# 初始化输出解析器，将模型的输出解析为字符串\n",
    "parser = StrOutputParser()\n",
    "# 构建一个链式处理流程，先使用提示模板处理输入，再经过模型生成输出，最后用解析器解析输出\n",
    "chain = prompt | model | parser\n",
    "\n",
    "\n",
    "async def async_stream():\n",
    "    # 调用链式处理流程的 stream 方法，传入 topic 参数，得到一个生成器\n",
    "    gen = chain.stream({\"topic\": \"天空\"})\n",
    "    # 将生成器包装成异步可迭代对象\n",
    "    async_gen = AsyncGeneratorWrapper(gen)\n",
    "    # 异步迭代生成器，逐块输出内容\n",
    "    async for chunk in async_gen:\n",
    "        # 打印每一块内容，以 | 作为分隔符，并立即刷新输出缓冲区\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "\n",
    "\n",
    "# 根据环境决定是否使用 asyncio.run\n",
    "try:\n",
    "    # 获取当前正在运行的事件循环\n",
    "    loop = asyncio.get_running_loop()\n",
    "    if loop.is_running():\n",
    "        # 如果事件循环正在运行，创建一个任务来运行异步函数\n",
    "        loop.create_task(async_stream())\n",
    "    else:\n",
    "        # 如果事件循环未运行，使用 asyncio.run 来运行异步函数\n",
    "        asyncio.run(async_stream())\n",
    "except RuntimeError:\n",
    "    # 如果获取运行的事件循环时出现错误，使用 asyncio.run 来运行异步函数\n",
    "    asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-8' coro=<async_stream() done, defined at C:\\Users\\33249\\AppData\\Local\\Temp\\ipykernel_11836\\4150857872.py:31> wait_for=<Future pending cb=[_chain_future.<locals>._call_check_cancel() at c:\\Users\\33249\\anaconda3\\envs\\xhd\\Lib\\asyncio\\futures.py:387, Task.task_wakeup()]>>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'country': {}}\n",
      "{'country': {'France': {}}}\n",
      "{'country': {'France': {'name': ''}}}\n",
      "{'country': {'France': {'name': 'France'}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 6}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 670}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 6700}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 670000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 6700000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': ''}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain'}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 4}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 440}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 4400}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 440000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 4400000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': ''}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Port'}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal'}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 6}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 672}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 6720}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 672000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 6720000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': ''}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan'}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 1}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 13}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 138}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 1380}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 13800}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 138000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 1380000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 13800000}}}\n",
      "{'country': {'France': {'name': 'France', 'population': 67000000}, 'Spain': {'name': 'Spain', 'population': 44000000}, 'Portugal': {'name': 'Portugal', 'population': 67200000}, 'Japan': {'name': 'Japan', 'population': 138000000}}}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "# 从 langchain_ollama 库导入 OllamaLLM 类，用于调用 Ollama 模型\n",
    "from langchain_ollama import OllamaLLM\n",
    "# 从 langchain_core.output_parsers 库导入 JsonOutputParser 类，用于将模型输出解析为 JSON 格式\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# 初始化 Ollama 语言模型，指定模型名称为 deepseek-r1:1.5b，模型服务的基础 URL 为 http://localhost:11434\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "# 创建一个链式调用，先使用模型进行推理，然后使用 JsonOutputParser 将输出解析为 JSON 格式\n",
    "chain = (model | JsonOutputParser())\n",
    "\n",
    "# 定义一个类，用于将同步生成器包装成异步生成器\n",
    "class AsyncGeneratorWrapper:\n",
    "    def __init__(self, gen):\n",
    "        # 初始化时接收一个同步生成器对象\n",
    "        self.gen = gen\n",
    "\n",
    "    def __aiter__(self):\n",
    "        # 使该类的实例成为异步可迭代对象\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        try:\n",
    "            # 使用 asyncio.to_thread 将同步的 next 调用放到单独的线程中执行，避免阻塞事件循环\n",
    "            return await asyncio.to_thread(next, self.gen)\n",
    "        except StopIteration:\n",
    "            # 当同步生成器耗尽时，抛出异步迭代结束的异常\n",
    "            raise StopAsyncIteration\n",
    "\n",
    "# 定义一个异步函数，用于流式处理模型的输出\n",
    "async def async_stream():\n",
    "    # 调用 chain 的 stream 方法，传入请求内容，得到一个同步生成器，该请求要求以特定 JSON 格式输出国家及人口列表\n",
    "    gen = chain.stream(\n",
    "        \"以JSON 格式输出法国、西班牙和日本的国家及人口列表。\"\n",
    "        '使用一个带有“countries”外部键的字典，其中包含国家列表。'\n",
    "        '每个国家都应该有键“name”和“population”。'\n",
    "    )\n",
    "    # 将同步生成器包装成异步生成器\n",
    "    async_gen = AsyncGeneratorWrapper(gen)\n",
    "    # 异步迭代异步生成器，逐块获取模型输出\n",
    "    async for text in async_gen:\n",
    "        # 打印每一块输出，并立即刷新输出缓冲区\n",
    "        print(text, flush=True)\n",
    "\n",
    "try:\n",
    "    # 获取当前正在运行的事件循环\n",
    "    loop = asyncio.get_running_loop()\n",
    "    if loop.is_running():\n",
    "        # 如果事件循环正在运行，创建一个任务来运行异步函数\n",
    "        loop.create_task(async_stream())\n",
    "    else:\n",
    "        # 如果事件循环未运行，使用 asyncio.run 来运行异步函数\n",
    "        asyncio.run(async_stream())\n",
    "except RuntimeError:\n",
    "    # 如果获取运行的事件循环时出现错误，使用 asyncio.run 来运行异步函数\n",
    "    asyncio.run(async_stream())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多线程调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def async_stream1():\n",
    "    # 调用链式处理流程的 stream 方法，传入 topic 参数，得到一个生成器\n",
    "    gen = chain.stream({\"topic\": \"天空\"})\n",
    "    # 将生成器包装成异步可迭代对象\n",
    "    async_gen = AsyncGeneratorWrapper(gen)\n",
    "    # 异步迭代生成器，逐块输出内容\n",
    "    async for chunk in async_gen:\n",
    "        # 打印每一块内容，以 | 作为分隔符，并立即刷新输出缓冲区\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "        \n",
    "async def async_stream2():\n",
    "    gen = chain.stream({\"topic\": \"大海\"})\n",
    "    async_gen = AsyncGeneratorWrapper(gen)\n",
    "    async for chunk in async_gen:\n",
    "        print(chunk, end=\"|\", flush=True)\n",
    "        \n",
    "async def main():\n",
    "    # 同步调用\n",
    "    # await async_stream1()\n",
    "    # await async_stream2()\n",
    "    \n",
    "    # 异步调用（两个协程，它们内部的操作都是\n",
    "    # 基于异步 I/O 等异步操作来实现的，所以它们代表的是两个协程，而不是线程。）\n",
    "    await asyncio.gather(async_stream1(), async_stream2())\n",
    "    \n",
    "try:\n",
    "    loop = asyncio.get_running_loop()\n",
    "    if loop.is_running():\n",
    "        loop.create_task(async_stream1())\n",
    "        loop.create_task(async_stream2())\n",
    "    else:\n",
    "        asyncio.run(main())\n",
    "except RuntimeError:\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 事件流"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'event': 'on_llm_start', 'data': {'input': 'hello'}, 'name': 'OllamaLLM', 'tags': [], 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '<think>'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '\\n\\n'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '</think>'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '\\n\\n'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': 'Hello'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '!'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' How'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' can'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' I'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' assist'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' you'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' today'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': '?'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ' 😊'}, 'parent_ids': []}, {'event': 'on_llm_stream', 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'data': {'chunk': ''}, 'parent_ids': []}, {'event': 'on_llm_end', 'data': {'output': {'generations': [[{'text': '<think>\\n\\n</think>\\n\\nHello! How can I assist you today? 😊', 'generation_info': {'model': 'deepseek-r1:1.5b', 'created_at': '2025-03-21T02:01:36.5025148Z', 'done': True, 'done_reason': 'stop', 'total_duration': 479357900, 'load_duration': 65624500, 'prompt_eval_count': 4, 'prompt_eval_duration': 4000000, 'eval_count': 16, 'eval_duration': 408000000, 'response': '', 'context': [151644, 14990, 151645, 151648, 271, 151649, 271, 9707, 0, 2585, 646, 358, 7789, 498, 3351, 30, 26525, 232]}, 'type': 'Generation'}]], 'llm_output': None}}, 'run_id': 'c32e4a2e-ecb3-4f2c-a8bc-8ff8e2fc121b', 'name': 'OllamaLLM', 'tags': [], 'metadata': {'ls_provider': 'ollama', 'ls_model_type': 'llm', 'ls_model_name': 'deepseek-r1:1.5b'}, 'parent_ids': []}]\n"
     ]
    }
   ],
   "source": [
    "# 从 langchain_ollama 库导入 OllamaLLM 类，用于与 Ollama 模型进行交互\n",
    "from langchain_ollama import OllamaLLM\n",
    "# 从 langchain_core.output_parsers 库导入 JsonOutputParser 类，用于将模型输出解析为 JSON 格式\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "# 导入 asyncio 库，用于实现异步编程\n",
    "import asyncio\n",
    "\n",
    "# 初始化 Ollama 语言模型，指定使用的模型名称为 deepseek - r1:1.5b\n",
    "# 并设置模型服务的基础 URL 为本地的 http://localhost:11434\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\", base_url=\"http://localhost:11434\")\n",
    "# 创建一个链式调用，先使用模型进行推理，然后使用 JsonOutputParser 将输出解析为 JSON 格式\n",
    "chain = (model | JsonOutputParser())\n",
    "\n",
    "# 定义一个类，用于将异步生成器包装成另一个异步可迭代对象\n",
    "# 虽然代码里目前未使用该类，但保留以作后续扩展用途\n",
    "class AsyncGeneratorWrapper:\n",
    "    def __init__(self, gen):\n",
    "        # 初始化时接收一个异步生成器对象\n",
    "        self.gen = gen\n",
    "\n",
    "    def __aiter__(self):\n",
    "        # 使该类的实例成为异步可迭代对象\n",
    "        return self\n",
    "\n",
    "    async def __anext__(self):\n",
    "        # 尝试获取异步生成器的下一个值\n",
    "        return await self.gen.__anext__()\n",
    "\n",
    "# 定义一个异步函数，用于流式处理模型的事件\n",
    "async def async_stream():\n",
    "    # 初始化一个空列表，用于存储模型返回的事件\n",
    "    events = []\n",
    "    # 异步迭代模型的事件流，该事件流是通过向模型输入 \"hello\" 并指定版本 \"v2\" 得到的\n",
    "    async for event in model.astream_events(input=\"hello\", version=\"v2\"):\n",
    "        # 将每个事件添加到 events 列表中\n",
    "        events.append(event)\n",
    "    # 打印存储所有事件的列表\n",
    "    print(events)\n",
    "\n",
    "try:\n",
    "    # 获取当前正在运行的事件循环\n",
    "    loop = asyncio.get_running_loop()\n",
    "    if loop.is_running():\n",
    "        # 如果事件循环正在运行，创建一个任务来运行异步函数\n",
    "        loop.create_task(async_stream())\n",
    "    else:\n",
    "        # 如果事件循环未运行，使用 asyncio.run 来运行异步函数\n",
    "        asyncio.run(async_stream())\n",
    "except RuntimeError:\n",
    "    # 如果获取运行的事件循环时出现错误，使用 asyncio.run 来运行异步函数\n",
    "    asyncio.run(async_stream())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
