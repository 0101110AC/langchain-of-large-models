{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains  import ConversationChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\",base_url=\"http://localhost:11434\",temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33249\\AppData\\Local\\Temp\\ipykernel_17688\\3540064232.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory() # 创建一个临时存储记忆的变量\n",
      "C:\\Users\\33249\\AppData\\Local\\Temp\\ipykernel_17688\\3540064232.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation =  ConversationChain(llm=model,memory=memory,verbose=False)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory() # 创建一个临时存储记忆的变量\n",
    "# 创建一个会话链径，并把llm和memory传入，verbose=False表示不打印对话内容\n",
    "conversation =  ConversationChain(llm=model,memory=memory,verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\n\\n</think>\\n\\nHello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For comprehensive details about our models and products, we invite you to consult our official documentation.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi,my name is pai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is  1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi,my name is pai\n",
      "AI: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Hello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For comprehensive details about our models and products, we invite you to consult our official documentation.\n",
      "Human: what is  1+1?\n",
      "AI: <think>\n",
      "Human: what is my name?\n",
      "AI: <think>\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi,my name is pai\\nAI: <think>\\n\\n</think>\\n\\nHello! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. For comprehensive details about our models and products, we invite you to consult our official documentation.\\nHuman: what is  1+1?\\nAI: <think>\\nHuman: what is my name?\\nAI: <think>\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({}) # 获取所有记忆变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains  import ConversationChain\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:1.5b\",base_url=\"http://localhost:11434\",temperature=0.1)\n",
    "\n",
    "# 创建一个 ConversationChain 对象，并传入模型和设置最大tokens长度\n",
    "memory = ConversationBufferMemory(llm=model,max_token_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"AI is what?\"}, {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"What is AI?\"}, \n",
    "{\"output\": \"AI is a type of machine that can do things that humans can't.\"})\n",
    "memory.save_context({\"inputs\":\"Chatbots is what?\"},\n",
    "{\"outputs\": \"Chatbots are a type of AI that can chat with humans.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: AI is what?\\nAI: Amazing!\\nHuman: What is AI?\\nAI: AI is a type of machine that can do things that humans can't.\\nHuman: Chatbots is what?\\nAI: Chatbots are a type of AI that can chat with humans.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果要加载之前保存的记忆，可以使用 load_memory_variables 方法\n",
    "# 如果最大tokens长度不够，它会删除这个对话最早的那部分信息，直到满足最大tokens长度\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 14\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 这里重新实例化是因为 LangChain 默认使用 GPT-2 分词器\n",
    "#（get_tokenizer() 函数会尝试加载 gpt2 模型的分词器），\n",
    "# 而你现在使用的是本地部署的模型（deepseek-r1:1.5b），有专门的分词器。\n",
    "# 运行这段代码需要魔法上网，运行一次下载完缓存文件，\n",
    "# 下次就不用再次使用魔法下载了（不清除缓存文件的情况下）\n",
    "\n",
    "# 加载 DeepSeek-R1 专用分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    trust_remote_code=True  # 需安装 transformers>=4.30.0\n",
    ")\n",
    "\n",
    "# 初始化 Ollama 模型时传入分词器\n",
    "model = OllamaLLM(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# 测试分词\n",
    "text = \"你好，世界！\"\n",
    "token_count = model.get_num_tokens(text)\n",
    "print(f\"Token count: {token_count}\")  # 输出：Token count: 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# 这里重新实例化是因为 LangChain 默认使用 GPT-2 分词器\n",
    "#（get_tokenizer() 函数会尝试加载 gpt2 模型的分词器），\n",
    "# 而你现在使用的是本地部署的模型（deepseek-r1:1.5b）\n",
    "model = OllamaLLM(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.1,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "schedule = \"There is a meeting at 5pm with your product team.\\\n",
    " You should prepare the presentation and discuss the new features.\\\n",
    " You should also check the email and respond to any important messages.\\\n",
    "The next meeting is with your team leader.\\\n",
    " You should discuss the progress of the project and any issues that need to be addressed.\\\n",
    " You should also prepare a report on the project progress.\"\n",
    " \n",
    "memory = ConversationSummaryBufferMemory(llm=model,max_token_limit=400)\n",
    "memory.save_context({\"input\": \"What is the schedule for today?\"}, {\"output\": \"have a meeting with your product team\"})\n",
    "memory.save_context({\"input\": \"What is the schedule for tomorrow?\"}, {\"output\": \"have a meeting with your team leader\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: What is the schedule for today?\\nAI: have a meeting with your product team\\nHuman: What is the schedule for tomorrow?\\nAI: have a meeting with your team leader'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What is the schedule for today?\n",
      "AI: have a meeting with your product team\n",
      "Human: What is the schedule for tomorrow?\n",
      "AI: have a meeting with your team leader\n",
      "Human: What is the schedule for today?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to figure out what the user is asking. They\\'ve asked about the schedule for today and then again for tomorrow, but only provided the answer for today. Hmm, maybe they\\'re confused or just not sure how to proceed.\\n\\nLet me think... The AI\\'s response was \"have a meeting with your product team\" for today and \"have a meeting with your team leader\" for tomorrow. That makes sense because those are common meetings people have on workdays. But when the user asks again about today, they might be expecting more details or different information.\\n\\nWait, maybe the AI is supposed to provide more specific details? Or perhaps it\\'s just repeating the same answers without adding anything new. I should consider if there\\'s any additional context or information that could help answer their question better. But based on what\\'s given, I can only go with the meetings mentioned.\\n\\nI wonder if the user is testing how the AI responds to repeated questions or if they\\'re trying to get more detailed information. Either way, I\\'ll stick to the answers provided and make sure my response is clear and helpful.\\n</think>\\n\\nThe schedule for today is a meeting with your product team, and for tomorrow, it\\'s a meeting with your team leader.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = ConversationChain(llm=model,memory=memory,verbose=True)\n",
    "\n",
    "conversation.predict(input=\"What is the schedule for today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
